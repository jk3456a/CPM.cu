# CPM.cu

<strong>ä¸­æ–‡ | [English Version](./README.md)</strong>

CPM.cu æ˜¯ä¸€ä¸ªé’ˆå¯¹ç«¯ä¾§å¤§æ¨¡å‹æ¨ç†è®¾è®¡çš„è½»é‡ã€é«˜æ•ˆçš„ CUDA æ¨ç†æ¡†æ¶ï¼Œæ ¸å¿ƒæ”¯æŒ **ç¨€ç–æ¶æ„**ã€**æŠ•æœºé‡‡æ ·** å’Œ **ä½ä½å®½é‡åŒ–** ç­‰å‰æ²¿æŠ€æœ¯åˆ›æ–°ã€‚

<div id="news"></div>

## ğŸ”¥ é¡¹ç›®è¿›å±•

- [2025.06.06] ä¸º [MiniCPM4](https://github.com/openbmb/minicpm) ä¼˜åŒ–ã€‚
    - æ”¯æŒ InfLLM-v2 æ³¨æ„åŠ›å†…æ ¸
    - æ”¯æŒ MTP å±‚çš„æ»‘åŠ¨çª—å£ï¼Œä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡å¤„ç†
    - æ”¯æŒ MTP å±‚çš„é‡åŒ–
- [2025.05.29] æ”¯æŒ [SpecMQuant](https://github.com/AI9Stars/SpecMQuant) çš„é‡åŒ–ã€‚
    - æ”¯æŒ LLM çš„ Marlin GPTQ å†…æ ¸
    - æ”¯æŒé‡åŒ– LLM çš„æŠ•æœºé‡‡æ ·
- [2025.03.01] åœ¨ [FR-Spec](https://github.com/thunlp/FR-Spec) å‘å¸ƒé¦–ä¸ªç‰ˆæœ¬ã€‚
    - é€Ÿåº¦æœ€å¿«çš„æŠ•æœºé‡‡æ ·å®ç°
    - æ”¯æŒ FR-Spec, åŸºäºè¯é¢‘ä¼˜åŒ–çš„æŠ•æœºé‡‡æ ·
    - æ”¯æŒ Flash-Attention ä¸­çš„æ ‘çŠ¶æŠ•æœºé‡‡æ ·éªŒè¯
    - æ”¯æŒé™æ€å†…å­˜ç®¡ç†å’Œå†…å­˜å¤ç”¨
    - æ”¯æŒè®¡ç®—èåˆå†…æ ¸
    - æ”¯æŒåˆ†å—é¢„å¡«å……
    - æ”¯æŒ CUDA Graph

<div id="demo"></div>

## æ•ˆæœæ¼”ç¤º

https://github.com/user-attachments/assets/ab36fd7a-485b-4707-b72f-b80b5c43d024

<div id="getstart"></div>

## å¿«é€Ÿå¼€å§‹

- [æ¡†æ¶å®‰è£…](#install)
- [Docker ä½¿ç”¨](#docker)
- [æ¨¡å‹æƒé‡](#modelweights)
- [å‘½ä»¤è¡Œæ¥å£ (CLI)](#cli)
- [OpenAI API æœåŠ¡](#openai-api)

<div id="install"></div>

## æ¡†æ¶å®‰è£…

### ä»æºç å®‰è£…

æœ¬åº“çš„æ„å»ºä¾èµ–äº PyTorch å’Œ Ninjaï¼Œè¯·åœ¨å®‰è£…æœ¬åº“å‰ç¡®ä¿å·²æ­£ç¡®å®‰è£…è¿™ä¸¤ä¸ªä¾èµ–ã€‚

```bash
git clone https://github.com/OpenBMB/CPM.cu.git --recursive
cd CPM.cu
pip install .
```

å¦‚é‡åˆ°å®‰è£…é—®é¢˜ï¼Œè¯·æ ¹æ®é”™è¯¯æç¤ºè¿›è¡Œè§£å†³ï¼Œæˆ–é€šè¿‡ GitHub Issues æäº¤é—®é¢˜åé¦ˆã€‚ä½ å¯ä»¥ä½¿ç”¨ `python setup.py --help-config` æŸ¥çœ‹æ›´å¤šå®‰è£…é…ç½®ä¿¡æ¯ã€‚

<div id="docker"></div>

## Docker ä½¿ç”¨

æˆ‘ä»¬æä¾›äº†é¢„æ„å»ºçš„ Docker é•œåƒï¼Œæ”¯æŒå¼€ç®±å³ç”¨çš„ GPU æ¨ç†ç¯å¢ƒã€‚

### å¿«é€Ÿå¼€å§‹

```bash
# æ‹‰å–é¢„æ„å»ºé•œåƒ
docker pull cpmcu/cpmcu:cuda12.6-release

# è¿è¡Œäº¤äº’å¼å®¹å™¨
docker run --gpus all -it cpmcu:cuda12.6-release

# å¯åŠ¨ API æœåŠ¡å™¨
docker run --gpus all -p 8000:8000 cpmcu:cuda12.6-release \
  python examples/minicpm4/start_server.py
```

### ç¦»çº¿ä½¿ç”¨ï¼ˆæ¨èï¼‰

```bash
# 1. åœ¨å®¿ä¸»æœºä¸‹è½½æ¨¡å‹
huggingface-cli download openbmb/MiniCPM4-8B --local-dir /path/to/model

# 2. æŒ‚è½½æ¨¡å‹ç›®å½•è¿è¡Œ
docker run --rm --gpus all \
  -v /path/to/model:/workspace/model \
  cpmcu:cuda12.6-release \
  bash -lc 'cd examples && python3 minicpm4/test_generate.py \
    --model-path /workspace/model \
    --prompt-text "ä½ å¥½" --num-generate 128 --use-stream false'
```

**è¯¦ç»†æ–‡æ¡£**: [Docker ç”¨æˆ·æŒ‡å—](doc/ch/docker_use.md)

<div id="modelweights"></div>

## å‡†å¤‡æ¨¡å‹

è¯·æŒ‰ç…§ [MiniCPM4 çš„ README](https://github.com/openbmb/minicpm) çš„è¯´æ˜ä¸‹è½½æ¨¡å‹æƒé‡ã€‚

<div id="example"></div>

## è¿è¡Œç¤ºä¾‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ¥å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CPM.cuã€‚

```bash
cd examples
python3 minicpm4/test_generate.py --prompt-file <è¾“å…¥æ–‡ä»¶è·¯å¾„>
```

å¦‚æœæ‚¨ä¸æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œè„šæœ¬å°†ä» OpenBMB çš„ Hugging Face ä»“åº“åŠ è½½æ¨¡å‹ã€‚
å¦‚æœä½ æƒ³ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œæˆ‘ä»¬æ¨èä¸ä¿®æ”¹æ‰€æœ‰æ¨¡å‹æ–‡ä»¶åå¹¶æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œè¿™æ ·å¯ä»¥é€šè¿‡-pæŒ‡å®šè¯¥ç›®å½•è¿è¡Œæ¨¡å‹ã€‚å¦åˆ™å»ºè®®ä¿®æ”¹ä»£ç ä¸­çš„è·¯å¾„ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨ --help äº†è§£æ›´å¤šå…³äºè„šæœ¬çš„åŠŸèƒ½ã€‚

æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªè„šæœ¬ï¼Œ`examples/long_prompt_gen.py`ï¼Œç”¨äºç”Ÿæˆé•¿ä»£ç æ€»ç»“ã€‚
è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ä»æœ¬ä»“åº“ä¸­æ”¶é›†ä»£ç ï¼Œå¹¶æç¤ºæ¨¡å‹"Summarize the code."ã€‚

```bash
cd examples
python3 long_prompt_gen.py # ç”Ÿæˆ prompt.txt (æ›´å¤šç»†èŠ‚è¯·è§ --help)
python3 minicpm4/test_generate.py --prompt-file ../prompt.txt
```

è¾“å‡ºåº”ä¸ºå¦‚ä¸‹æ ¼å¼ï¼š

```bash
Generated text (streaming output):
--------------------------------------------------
Prefilling: 100.0% (106850/106850 tokens) @ 6565.3 tokens/s - Complete!

<Generated Output HERE>
==================================================
Stream Generation Summary:
==================================================
Prefill length: 106850
Prefill time: 16.36 s
Prefill tokens/s: 6530.77
Mean accept length: 2.50
Decode length: 118
Decode time: 0.76 s
Decode tokens/s: 154.59
```

å…¶ä¸­ï¼š

- `Prefill` (è¾“å…¥) å’Œ `Decode` (è¾“å‡º) é€Ÿåº¦é€šè¿‡ï¼ˆé•¿åº¦ã€æ—¶é—´å’Œ token/sï¼‰è¾“å‡ºã€‚
- `Mean accept length` (å¹³å‡æ¥å—é•¿åº¦) æ˜¯ä½¿ç”¨æŠ•æœºé‡‡æ ·æ—¶æ¥å— token çš„å¹³å‡é•¿åº¦ã€‚

<div id="cli"></div>

## å‘½ä»¤è¡Œæ¥å£ (CLI)

å¯¹äºéœ€è¦æ›´ç²¾ç»†åŒ–æ§åˆ¶æ¨ç†å‚æ•°ï¼ˆå¦‚æ¸©åº¦ã€ç”Ÿæˆé•¿åº¦ç­‰ï¼‰çš„ç”¨æˆ·ï¼Œæˆ‘ä»¬æ¨èç›´æ¥ä½¿ç”¨ `cpmcu.cli` æ¨¡å—ã€‚è¿™æ˜¯è¿›è¡Œè¯¦ç»†é…ç½®å’Œæµ‹è¯•æœ€çµæ´»çš„æ–¹å¼ã€‚

ä½ å¯ä»¥é€šè¿‡ `python -m cpmcu.cli -h` æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ã€‚

**ä½¿ç”¨ç¤ºä¾‹:**

ä»¥ä¸‹å‘½ä»¤å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ CLI æ¥å£ï¼Œå¹¶è®¾ç½® `temperature` ä¸º `0.7`ï¼š

```bash
python -m cpmcu.cli \
    --model-path openbmb/MiniCPM-Llama3-V-2_5-int4 \
    --prompt-text "ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦" \
    --temperature 0.7 \
    --use-stream true
```

<div id="openai-api"></div>

## OpenAI API æœåŠ¡

CPM.cu æ”¯æŒéƒ¨ç½²ä¸ºä¸€ä¸ªä¸ OpenAI API å…¼å®¹çš„æœåŠ¡ï¼Œæ–¹ä¾¿ä¸ç°æœ‰çš„ç”Ÿæ€ç³»ç»Ÿé›†æˆã€‚

### 1. å¯åŠ¨æœåŠ¡

å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨ï¼ˆå‚æ•°ä¸ `examples/minicpm4/test_generate.py` ç›¸åŒï¼‰ï¼š

```bash
python examples/minicpm4/start_server.py [options]
```
æœåŠ¡å¯åŠ¨åï¼Œé»˜è®¤ç›‘å¬åœ¨ `http://localhost:8000`ã€‚ä½ å¯ä»¥é€šè¿‡ `--host` å’Œ `--port` å‚æ•°æ¥ä¿®æ”¹ã€‚

### 2. æµ‹è¯•æœåŠ¡

ä½ å¯ä»¥ä½¿ç”¨ `examples/test_openai_api.py` è„šæœ¬æ¥æµ‹è¯•æœåŠ¡ã€‚è¯¥è„šæœ¬æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§æ¨¡å¼ï¼Œå¹¶å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ã€‚

**åŸºç¡€ç”¨æ³•:**

```bash
python examples/test_openai_api.py
```

**æµ‹è¯•ä¸åŒæ¸©åº¦:**

è¯¥è„šæœ¬ä¹Ÿæ”¯æŒ `--temperature` å‚æ•°ï¼Œæ–¹ä¾¿ä½ æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒæ¸©åº¦ä¸‹çš„ç”Ÿæˆæ•ˆæœã€‚

```bash
python examples/test_openai_api.py --temperature 0.5 [--no-stream]
```

ç›®å‰æœåŠ¡ä»…æ”¯æŒ `/v1/chat/completions` æ¥å£ï¼Œè¯·æ±‚ä¸­çš„ `model` å­—æ®µä¼šè¢«å¿½ç•¥ã€‚

## ä»£ç ç»“æ„

```bash
CPM.cu/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ flash_attn/ # attention: ç¨€ç–, æŠ•æœºæ ‘çŠ¶éªŒè¯ç­‰
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ minicpm4/ # minicpm4 æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ w4a16_gptq_marlin/ # Marlin GPTQ è®¡ç®—å†…æ ¸
â”‚   â”‚   â””â”€â”€ ... # é€šç”¨å±‚
â”‚   â”œâ”€â”€ entry.cu # pybind: ç»‘å®š CUDA å’Œ Python
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cpmcu/ # Python æ¥å£
â””â”€â”€ ...
```
## æ›´å¤š

### è¯é¢‘æ–‡ä»¶ç”Ÿæˆ
æˆ‘ä»¬æä¾›äº†FR-Specçš„è¯é¢‘ç”Ÿæˆè„šæœ¬ï¼Œä½äº"scripts/fr_spec/gen_fr_index.py"ï¼Œè¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼š
```bash
python scripts/fr_spec/gen_fr_index.py --model_path <your modelpath>
```
ä½ å¯ä»¥ä¿®æ”¹ä»£ç ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ã€‚å¦‚æœä½ çš„ä»»åŠ¡æ˜¯ç‰¹å®šå‚ç›´é¢†åŸŸï¼Œæ ¹æ®é¢†åŸŸæ„é€ è¯é¢‘å¯¹é€Ÿåº¦æå‡æœ‰æ˜¾è‘—æ”¶ç›Šã€‚

### GPTQè½¬Marlinæ ¼å¼
æˆ‘ä»¬æä¾›äº†GPTQé‡åŒ–æ¨¡å‹è½¬Marlinæ ¼å¼çš„è½¬æ¢è„šæœ¬ï¼Œä½äº"scripts/model_convert/gptq2marlin.py"ï¼Œè¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼š
```bash
python scripts/model_convert/gptq2marlin.py \
    --src <gptq_model_path> \
    --dst <marlin_model_path>
```
è¯¥è„šæœ¬æ”¯æŒMiniCPMã€Llamaå’ŒEAGLEæ ¼å¼ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹å¹¶è¿›è¡Œç›¸åº”è½¬æ¢ã€‚

## è‡´è°¢

æˆ‘ä»¬çš„ `src/flash_attn` æ–‡ä»¶å¤¹åŸºäº [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/v2.6.3/csrc/flash_attn) å¹¶è¿›è¡Œäº†ä¿®æ”¹ã€‚

æˆ‘ä»¬ä»ä»¥ä¸‹ä»“åº“ä¸­è·å–äº†å®ç°çµæ„Ÿï¼š

- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [Block-Sparse-Attention](https://github.com/mit-han-lab/Block-Sparse-Attention)
- [vLLM](https://github.com/vllm-project/vllm)
- [SGLang](https://github.com/sgl-project/sglang)

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

```
@article{zhao2025fr,
  title={FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling},
  author={Zhao, Weilin and Pan, Tengyu and Han, Xu and Zhang, Yudi and Sun, Ao and Huang, Yuxiang and Zhang, Kaihuo and Zhao, Weilun and Li, Yuxuan and Wang, Jianyong and others},
  journal={arXiv preprint arXiv:2502.14856},
  year={2025}
}

@article{zhang2025specmqaunt,
  title={Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design},
  author={Zhang, Yudi and Zhao, Weilin and Han, Xu and Zhao, Tiejun and Xu, Wang and Cao, Hailong and Zhu, Conghui},
  journal={arXiv preprint arXiv:2505.22179},
  year={2025}
}

@article{minicpm4,
  title={MiniCPM4: Ultra-Efficient LLMs on End Devices},
  author={MiniCPM},
  year={2025}
}