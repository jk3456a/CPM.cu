from eagle.model.ea_model import EaModel
from transformers import AutoTokenizer
import torch
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="/cache/lizhen/repos/temp-cpm/CPM.cu/models/Meta-Llama-3.1-8B-Instruct")
    parser.add_argument("--eagle_model", type=str, default="/cache/lizhen/repos/temp-cpm/CPM.cu/models/EAGLE-LLaMA3.1-Instruct-8B")
    parser.add_argument("--prompt", type=str, default="who are you?")
    parser.add_argument("--max_new_tokens", type=int, default=100)
    parser.add_argument("--temperature", type=float, default=0.0)  # æ”¹ä¸º0.0è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
    
    # ===== ğŸ¯ Promptæ ¼å¼é€‰é¡¹ =====
    parser.add_argument("--use_chat_template", action="store_true", default=True,
                       help="ä½¿ç”¨LLaMAçš„chat templateæ ¼å¼")
    parser.add_argument("--use_direct_prompt", action="store_true", 
                       help="ä½¿ç”¨ç›´æ¥çš„æŒ‡ä»¤æ ¼å¼")
    parser.add_argument("--no_chat_template", action="store_true",
                       help="ä½¿ç”¨åŸç”Ÿpromptæ ¼å¼ï¼ˆä¸ä½¿ç”¨chat templateï¼‰")
    
    # ===== ğŸ¯ EAGLE Speculative Decodingå‚æ•° =====
    parser.add_argument("--speculative-num-draft-tokens", type=int, default=32, 
                       help="æ€»çš„draft tokenæ•°é‡ (å¯¹åº”total_token)")
    parser.add_argument("--speculative-num-steps", type=int, default=5,
                       help="ç”Ÿæˆæ ‘çš„æ·±åº¦/æ­¥æ•° (å¯¹åº”depth)")
    parser.add_argument("--speculative-eagle-topk", type=int, default=8,
                       help="æ¯å±‚ç”Ÿæˆçš„å€™é€‰tokenæ•°é‡ (å¯¹åº”top_k)")
    parser.add_argument("--threshold", type=float, default=1.0,
                       help="æ¥å—é˜ˆå€¼")
    args = parser.parse_args()

    print(f"åŠ è½½ base æ¨¡å‹: {args.base_model}")
    print(f"åŠ è½½ EAGLE è‰ç¨¿æ¨¡å‹: {args.eagle_model}")
    print(f"ğŸ¯ EAGLEå‚æ•°è®¾ç½®:")
    print(f"  - Draft Tokensæ•°é‡: {args.speculative_num_draft_tokens}")
    print(f"  - ç”Ÿæˆæ­¥æ•°: {args.speculative_num_steps}")  
    print(f"  - TopK: {args.speculative_eagle_topk}")
    print(f"  - é˜ˆå€¼: {args.threshold}")
    print(f"  - Temperature: {args.temperature}")

    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)
    model = EaModel.from_pretrained(
        use_eagle3=False,  # å¯¹äºyuhuili/EAGLE-LLaMA3.1-Instruct-8Bæ¨¡å‹ä½¿ç”¨False
        base_model_path=args.base_model,
        ea_model_path=args.eagle_model,
        # ===== ğŸ¯ EAGLEæ ¸å¿ƒå‚æ•°è®¾ç½® =====
        total_token=args.speculative_num_draft_tokens,
        depth=args.speculative_num_steps,
        top_k=args.speculative_eagle_topk,
        threshold=args.threshold,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    model.eval()
    
    # ä½¿ç”¨å¤–éƒ¨tokenizeræ›¿æ¢æ¨¡å‹å†…éƒ¨çš„tokenizer
    model.tokenizer = tokenizer

    # ===== ğŸ¯ æ”¹è¿›Promptæ ¼å¼ =====
    if args.use_direct_prompt:
        # ä½¿ç”¨æ›´ç›´æ¥çš„æŒ‡ä»¤æ ¼å¼
        formatted_prompt = f"Please answer this question directly and concisely: {args.prompt}\nAnswer:"
        print(f"ğŸ“ ä½¿ç”¨ç›´æ¥æŒ‡ä»¤æ ¼å¼")
    elif args.no_chat_template:
        # ä½¿ç”¨åŸç”Ÿpromptæ ¼å¼
        formatted_prompt = args.prompt
        print(f"ğŸ“ ä½¿ç”¨åŸç”Ÿpromptæ ¼å¼")
    elif args.use_chat_template:
        # ä½¿ç”¨LLaMAçš„chat template
        messages = [
            {"role": "user", "content": args.prompt}
        ]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        print(f"ğŸ“ ä½¿ç”¨Chat Templateæ ¼å¼")
    else:
        # é»˜è®¤ä½¿ç”¨åŸç”Ÿpromptæ ¼å¼
        formatted_prompt = args.prompt
        print(f"ğŸ“ ä½¿ç”¨åŸç”Ÿpromptæ ¼å¼ï¼ˆé»˜è®¤ï¼‰")
    
    print(f"ğŸ”¤ æœ€ç»ˆprompt: {repr(formatted_prompt)}")

    input_ids = tokenizer([formatted_prompt]).input_ids
    input_ids = torch.tensor(input_ids).cuda()
    
    # è®°å½•åŸå§‹è¾“å…¥é•¿åº¦
    input_length = input_ids.shape[1]

    output_ids = model.eagenerate(
        input_ids,
        temperature=args.temperature,
        max_new_tokens=args.max_new_tokens
    )

    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    generated_tokens = output_ids[0, input_length:]  # è·³è¿‡åŸå§‹è¾“å…¥éƒ¨åˆ†
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # åŒæ—¶æ˜¾ç¤ºå®Œæ•´è¾“å‡ºå’Œä»…ç”Ÿæˆéƒ¨åˆ†
    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    print("\n" + "="*60)
    print("ğŸ“Š ç”Ÿæˆç»“æœ:")
    print("="*60)
    print("âœ… å®Œæ•´è¾“å‡ºï¼ˆåŒ…å«promptï¼‰ï¼š", full_output)
    print("\nğŸ¯ ä»…ç”Ÿæˆå†…å®¹ï¼š", generated_text)
